
**TS Tokenizer** is a hybrid tokenizer designed specifically for tokenizing Turkish texts.
It uses a hybrid (lexicon-based and rule-based) approach to split text into tokens.


### Key Features:
- **Hybrid Tokenization**: Combines lexicon-based and rule-based techniques to tokenize complex Turkish texts with precision.
- **Special Token Handling**: Detects and processes mentions, hashtags, emails, URLs, dates, numbers, smileys, emoticons, and more.
- **Configurable Outputs**: Offers multiple output formats, including plain tokens, tagged tokens, tokenized lines, and tagged lines, to suit diverse NLP workflows.
- **Multi-core Processing**: Speeds up tokenization for large files with parallel processing.
- **Preprocess Handling**: Handles corrupted Turkish text and punctuation gracefully using built-in fixes.
- **Command-Line Friendly**: Use it directly from the terminal for file-based or piped input workflows.


On natural language processing (NLP), information retrieval, or text mining for Turkish, **TS Tokenizer** offers
a reliable solution for tokenization.

---

## Installation

You can install the ts-tokenizer package using pip.
```bash
pip install ts-tokenizer
```
Ensure you have Python 3.9 or higher installed on your system.

You can update current version using pip
```bash
pip install --upgrade ts-tokenizer
```
---

To remove package, use:
```bash
pip uninstall ts-tokenizer
```
You can also clone the repo locally.
```bash
git clone https://github.com/tanerim/ts_tokenizer.git
cd ts-tokenizer
pip install -e .

```

---

## License
This project is licensed under the MIT License. See the [LICENSE](https://github.com/tanerim/ts_tokenizer/blob/main/LICENSE) file for details.


## Command line tool

You can use TS Tokenizer directly from the command line for both file inputs and pipeline processing:
## Tokenize from a File:
```bash
ts-tokenizer input.txt
```
or
```bash
cat input.txt | ts-tokenizer
```
or
```bash
zcat input.txt.gz | ts-tokenizer
```
---

## Help

Get detailed help for available options using:

| Argument             | Short | Description                                                                                     | Default       |
|----------------------|-------|-------------------------------------------------------------------------------------------------|---------------|
| `--output`           | `-o`  | Specify the output format: `tokenized`, `lines`, `tagged`, `tagged_lines`.                      | `tokenized`   |
| `--num-workers`      | `-n`  | Set the number of parallel workers for processing.                                              | `CPU cores-1` |
| `--verbose`          | `-v`  | Enable verbose mode to display additional processing details.                                   | Disabled      |
| `--version`          | `-V`  | Display the current version of `ts-tokenizer`.                                                 | N/A           |
| `--help`             | `-h`  | Show the help message and exit.   

---

## CLI Arguments

You can specify the output format using the -o option:

- **tokenized (default):** Returns plain tokens, one per line.
- **tagged:** Returns tokens with their tags.
- **lines:** Returns tokenized lines as lists.
- **tagged_lines:** Returns tokenized lines as a list of tuples (token, tag).

```bash
input_text = "Queen , 31.10.1975 tarihinde Ã§Ä±kardÄ±Ã°Ä± A Night at the Opera albÃƒÂ¼mÃƒÂ¼yle dÃƒÂ¼nya mÃƒÂ¼ziÃ°ini deÃ°iÃ¥Ã¿tirdi ."

$ ts-tokenizer input_text

Queen
,
31.10.1975
tarihinde
Ã§Ä±kardÄ±ÄŸÄ±
A
Night
at
the
Opera
albÃ¼mÃ¼yle
dÃ¼nya
mÃ¼ziÄŸini
deÄŸiÅŸtirdi
.
```

Note that **tags are not part-of-speech tags** but they define the given string.
```bash
$ ts-tokenizer -o tagged input.txt

Queen	English_Word
,	Punc
31.10.1975	Date
tarihinde	Valid_Word
Ã§Ä±kardÄ±ÄŸÄ±	Valid_Word
A	OOV
Night	English_Word
at	Valid_Word
the	English_Word
Opera	Valid_Word
albÃ¼mÃ¼yle	Valid_Word
dÃ¼nya	Valid_Word
mÃ¼ziÄŸini	Valid_Word
deÄŸiÅŸtirdi	Valid_Word
.	Punc
```

The other two arguments are "lines" and "tagged_lines".
The "lines" parameter reads input file line-by-line and returns a list for each line. Note that each line is defined by end-of-line markers in the given text.
```bash
$ ts-tokenizer -o lines input.txt

['Queen', ',', '31.10.1975', 'tarihinde', 'Ã§Ä±kardÄ±ÄŸÄ±', 'A', 'Night', 'at', 'the', 'Opera', 'albÃ¼mÃ¼yle', 'dÃ¼nya', 'mÃ¼ziÄŸini', 'deÄŸiÅŸtirdi', '.']
```

The "tagged_lines" parameter reads input file line-by-line and returns a list of tuples for each line. Note that each line is defined by end-of-line markers in the given text.
```bash
$ ts-tokenizer -o tagged_lines input.txt

 [('Queen', 'English_Word'), (',', 'Punc'), ('31.10.1975', 'Date'), ('tarihinde', 'Valid_Word'), ('Ã§Ä±kardÄ±ÄŸÄ±', 'Valid_Word'), ('A', 'OOV'), ('Night', 'English_Word'), ('at', 'Valid_Word'), ('the', 'English_Word'), ('Opera', 'Valid_Word'), ('albÃ¼mÃ¼yle', 'Valid_Word'), ('dÃ¼nya', 'Valid_Word'), ('mÃ¼ziÄŸini', 'Valid_Word'), ('deÄŸiÅŸtirdi', 'Valid_Word'), ('.', 'Punc')]
```
---

## Parallel Processing
Use the -n option to set the number of parallel workers:
```bash
$ ts-tokenizer -n 2 -o tagged input_file
```

By default, TS Tokenizer uses [number of CPU cores - 1].

---

## Using CLI Arguments with pipelines

You can use TS Tokenizer in bash pipelines, such as counting word frequencies:

Following sample returns calculated  frequencies for the given file:
```bash
$ ts-tokenizer input.txt | sort | uniq -c | sort -n
```

---

To count tags:
```bash
$ ts-tokenizer -o tagged input.txt | cut -f2 | sort | uniq -c

1 Date
3 English_Word
2 Hashtag
2 Mention
1 Multi_Hyphenated
3 Numbered_Title
1 OOV
9 Punc
1 Single_Hyphenated
17 Valid_Word
```

To find a specific tag following command could be used.
```bash
$ ts-tokenizer -o tagged input.txt | cut -f1,2 | grep "Web_URL"

www.wikipedia.org	Web_URL
www.wim-wenders.com	Web_URL
www.winterwar.com.	Web_URL
www.wissenschaft.de:	Web_URL
www.wittingen.de	Web_URL
www.wlmqradio.com	Web_URL
www.worldstadiums.com	Web_URL
www.worldstatesmen.org	Web_URL
```
---

# Classes

Below are samples to implement ts-tokenizer in Python

## TSTokenizer

**tokenized** : Outputs a list of plain tokens extracted from the input text.

```python
from ts_tokenizer.tokenizer import TSTokenizer
Single_Line_Sample = "ParÃƒÂ§a ve bÃƒÂ¼tÃƒÂ¼n iliÃ¥Ã¿kisi her zaman iÃ¥Ã¿levsel deÃ°ildir."
simple_tokens = TSTokenizer.ts_tokenize(Single_Line_Sample, output_format="tokenized")
if simple_tokens is None:
    pass
else:
    for token in simple_tokens:
        print(token)
```
Generated output is as follows:

```bash
ParÃ§a
ve
bÃ¼tÃ¼n
iliÅŸkisi
her
zaman
iÅŸlevsel
deÄŸildir
```
**tagged** : Outputs tokens with associated tags. Please note that these are not POSTags.
Check TokenHandler below for tag set.

```python
tagged_tokens = TSTokenizer.ts_tokenize(Single_Line_Sample, output_format="tagged")
if tagged_tokens is None:
    pass
else:
    for token in tagged_tokens:
        print(token)
```
Generated output is as follows:

```bash
ParÃ§a	Valid_Word
ve	Valid_Word
bÃ¼tÃ¼n	Valid_Word
iliÅŸkisi	Valid_Word
her	Valid_Word
zaman	Valid_Word
iÅŸlevsel	Valid_Word
deÄŸildir	Valid_Word
.	Punc

```

**lines** : Maintains the structure of the input text, with each line's tokens grouped together.
Please note that "line" is defined with end-of-line markers.

```python
from ts_tokenizer.tokenizer import TSTokenizer
Multi_Line_Sample = """
ATATÃœRK'Ã¼n GENÃ‡LÃÃE HÃTABESÃ 
Ey TÃ¼rk genÃ§liÃ°i! Birinci vazifen, TÃ¼rk istiklÃ¢lini, TÃ¼rk Cumhuriyet'ini, ilelebet, muhafaza ve mÃ¼dafaa etmektir. 
Mevcudiyetinin ve istikbalinin yegÃ¢ne temeli budur. 
Bu temel, senin, en kÃ½ymetli hazinendir.
Ãstikbalde dahi, seni bu hazineden mahrum etmek isteyecek, dahilÃ® ve haricÃ® bedhahlarÃ½n olacaktÃ½r. 
Bir gÃ¼n, istiklÃ¢l ve cumhuriyeti mÃ¼dafaa mecburiyetine dÃ¼Ã¾ersen, vazifeye atÃ½lmak iÃ§in, iÃ§inde bulunacaÃ°Ã½n vaziyetin imkÃ¢n ve Ã¾eraitini dÃ¼Ã¾Ã¼nmeyeceksin! 
Bu imkÃ¢n ve Ã¾erait, Ã§ok nÃ¢mÃ¼sait bir mahiyette tezahÃ¼r edebilir. 
ÃstiklÃ¢l ve cumhuriyetine kastedecek dÃ¼Ã¾manlar, bÃ¼tÃ¼n dÃ¼nyada emsali gÃ¶rÃ¼lmemiÃ¾ bir galibiyetin mÃ¼messili olabilirler. 
Cebren ve hile ile aziz vatanÃ½n, bÃ¼tÃ¼n kaleleri zaptedilmiÃ¾, bÃ¼tÃ¼n tersanelerine girilmiÃ¾, bÃ¼tÃ¼n ordularÃ½ daÃ°Ã½tÃ½lmÃ½Ã¾ ve memleketin her kÃ¶Ã¾esi bilfiil iÃ¾gal edilmiÃ¾ olabilir. 
BÃ¼tÃ¼n bu Ã¾eraitten daha elÃ®m ve daha vahim olmak Ã¼zere, memleketin dahilinde, iktidara sahip olanlar gaflet ve dalÃ¢let ve hattÃ¢ hÃ½yanet iÃ§inde bulunabilirler.
Hatta bu iktidar sahipleri Ã¾ahsÃ® menfaatlerini, mÃ¼stevlilerin siyasÃ® emelleriyle tevhit edebilirler.
Millet, fakruzaruret iÃ§inde harap ve bÃ®tap dÃ¼Ã¾mÃ¼Ã¾ olabilir. Ey TÃ¼rk istikbalinin evladÃ½! ÃÃ¾te, bu ahval ve Ã¾erait iÃ§inde dahi, vazifen; TÃ¼rk istiklÃ¢l ve cumhuriyetini kurtarmaktÃ½r! 
MuhtaÃ§ olduÃ°un kudret, damarlarÃ½ndaki asil kanda, mevcuttur!
"""
line_tokens = TSTokenizer.ts_tokenize(Multi_Line_Sample, output_format="lines")
if line_tokens is None:
    pass
else:
    for token in line_tokens:
        print(token)
```
Generated output is as follows:
```text
ATATÃœRK'Ã¼n GENÃ‡LÄ°ÄE HÄ°TABESÄ°
Ey TÃ¼rk genÃ§liÄŸi ! Birinci vazifen , TÃ¼rk istiklÃ¢lini , TÃ¼rk Cumhuriyet'ini , ilelebet , muhafaza ve mÃ¼dafaa etmektir .
Mevcudiyetinin ve istikbalinin yegÃ¢ne temeli budur .
Bu temel , senin , en kÄ±ymetli hazinendir .
Ä°stikbalde dahi , seni bu hazineden mahrum etmek isteyecek , dahilÃ® ve haricÃ® bedhahlarÄ±n olacaktÄ±r .
Bir gÃ¼n , istiklÃ¢l ve cumhuriyeti mÃ¼dafaa mecburiyetine dÃ¼ÅŸersen , vazifeye atÄ±lmak iÃ§in , iÃ§inde bulunacaÄŸÄ±n vaziyetin imkÃ¢n ve ÅŸeraitini dÃ¼ÅŸÃ¼nmeyeceksin !
Bu imkÃ¢n ve ÅŸerait , Ã§ok nÃ¢mÃ¼sait bir mahiyette tezahÃ¼r edebilir .
Ä°stiklÃ¢l ve cumhuriyetine kastedecek dÃ¼ÅŸmanlar , bÃ¼tÃ¼n dÃ¼nyada emsali gÃ¶rÃ¼lmemiÅŸ bir galibiyetin mÃ¼messili olabilirler .
Cebren ve hile ile aziz vatanÄ±n , bÃ¼tÃ¼n kaleleri zaptedilmiÅŸ , bÃ¼tÃ¼n tersanelerine girilmiÅŸ , bÃ¼tÃ¼n ordularÄ± daÄŸÄ±tÄ±lmÄ±ÅŸ ve memleketin her kÃ¶ÅŸesi bilfiil iÅŸgal edilmiÅŸ olabilir .
BÃ¼tÃ¼n bu ÅŸeraitten daha elÃ®m ve daha vahim olmak Ã¼zere , memleketin dahilinde , iktidara sahip olanlar gaflet ve dalÃ¢let ve hattÃ¢ hÄ±yanet iÃ§inde bulunabilirler .
Hatta bu iktidar sahipleri ÅŸahsÃ® menfaatlerini , mÃ¼stevlilerin siyasÃ® emelleriyle tevhit edebilirler .
Millet , fakruzaruret iÃ§inde harap ve bÃ®tap dÃ¼ÅŸmÃ¼ÅŸ olabilir . Ey TÃ¼rk istikbalinin evladÄ± ! Ä°ÅŸte , bu ahval ve ÅŸerait iÃ§inde dahi , vazifen ; TÃ¼rk istiklÃ¢l ve cumhuriyetini kurtarmaktÄ±r !
MuhtaÃ§ olduÄŸun kudret , damarlarÄ±ndaki asil kanda , mevcuttur !
```

**tagged_lines**: Same as lines but includes tags for each token.

```python 
tagged_line_tokens = TSTokenizer.ts_tokenize(Multi_Line_Sample, output_format="tagged_lines")
if tagged_line_tokens is None:
    pass
else:
    for token in tagged_line_tokens:
        print(token)
```
Generated output is as follows:
```bash
[("ATATÃœRK'Ã¼n", 'Apostrophed'), ('GENÃ‡LÄ°ÄE', 'Valid_Word'), ('HÄ°TABESÄ°', 'Valid_Word')]
[('Ey', 'Valid_Word'), ('TÃ¼rk', 'Valid_Word'), ('genÃ§liÄŸi', 'Valid_Word'), ('!', 'Punc'), ('Birinci', 'Valid_Word'), ('vazifen', 'Valid_Word'), (',', 'Punc'), ('TÃ¼rk', 'Valid_Word'), ('istiklÃ¢lini', 'Valid_Word'), (',', 'Punc'), ('TÃ¼rk', 'Valid_Word'), ("Cumhuriyet'ini", 'Apostrophed'), (',', 'Punc'), ('ilelebet', 'Valid_Word'), (',', 'Punc'), ('muhafaza', 'Valid_Word'), ('ve', 'Valid_Word'), ('mÃ¼dafaa', 'Valid_Word'), ('etmektir', 'Valid_Word'), ('.', 'Punc')]
[('Mevcudiyetinin', 'Valid_Word'), ('ve', 'Valid_Word'), ('istikbalinin', 'Valid_Word'), ('yegÃ¢ne', 'Valid_Word'), ('temeli', 'Valid_Word'), ('budur', 'Valid_Word'), ('.', 'Punc')]
[('Bu', 'Valid_Word'), ('temel', 'Valid_Word'), (',', 'Punc'), ('senin', 'Valid_Word'), (',', 'Punc'), ('en', 'Valid_Word'), ('kÄ±ymetli', 'Valid_Word'), ('hazinendir', 'Valid_Word'), ('.', 'Punc')]
[('Ä°stikbalde', 'Valid_Word'), ('dahi', 'Valid_Word'), (',', 'Punc'), ('seni', 'Valid_Word'), ('bu', 'Valid_Word'), ('hazineden', 'Valid_Word'), ('mahrum', 'Valid_Word'), ('etmek', 'Valid_Word'), ('isteyecek', 'Valid_Word'), (',', 'Punc'), ('dahilÃ®', 'Valid_Word'), ('ve', 'Valid_Word'), ('haricÃ®', 'Valid_Word'), ('bedhahlarÄ±n', 'Valid_Word'), ('olacaktÄ±r', 'Valid_Word'), ('.', 'Punc')]
[('Bir', 'Valid_Word'), ('gÃ¼n', 'Valid_Word'), (',', 'Punc'), ('istiklÃ¢l', 'Valid_Word'), ('ve', 'Valid_Word'), ('cumhuriyeti', 'Valid_Word'), ('mÃ¼dafaa', 'Valid_Word'), ('mecburiyetine', 'Valid_Word'), ('dÃ¼ÅŸersen', 'Valid_Word'), (',', 'Punc'), ('vazifeye', 'Valid_Word'), ('atÄ±lmak', 'Valid_Word'), ('iÃ§in', 'Valid_Word'), (',', 'Punc'), ('iÃ§inde', 'Valid_Word'), ('bulunacaÄŸÄ±n', 'Valid_Word'), ('vaziyetin', 'Valid_Word'), ('imkÃ¢n', 'Valid_Word'), ('ve', 'Valid_Word'), ('ÅŸeraitini', 'Valid_Word'), ('dÃ¼ÅŸÃ¼nmeyeceksin', 'Valid_Word'), ('!', 'Punc')]
[('Bu', 'Valid_Word'), ('imkÃ¢n', 'Valid_Word'), ('ve', 'Valid_Word'), ('ÅŸerait', 'Valid_Word'), (',', 'Punc'), ('Ã§ok', 'Valid_Word'), ('nÃ¢mÃ¼sait', 'Valid_Word'), ('bir', 'Valid_Word'), ('mahiyette', 'Valid_Word'), ('tezahÃ¼r', 'Valid_Word'), ('edebilir', 'Valid_Word'), ('.', 'Punc')]
[('Ä°stiklÃ¢l', 'Valid_Word'), ('ve', 'Valid_Word'), ('cumhuriyetine', 'Valid_Word'), ('kastedecek', 'Valid_Word'), ('dÃ¼ÅŸmanlar', 'Valid_Word'), (',', 'Punc'), ('bÃ¼tÃ¼n', 'Valid_Word'), ('dÃ¼nyada', 'Valid_Word'), ('emsali', 'Valid_Word'), ('gÃ¶rÃ¼lmemiÅŸ', 'Valid_Word'), ('bir', 'Valid_Word'), ('galibiyetin', 'Valid_Word'), ('mÃ¼messili', 'Valid_Word'), ('olabilirler', 'Valid_Word'), ('.', 'Punc')]
[('Cebren', 'Valid_Word'), ('ve', 'Valid_Word'), ('hile', 'Valid_Word'), ('ile', 'Valid_Word'), ('aziz', 'Valid_Word'), ('vatanÄ±n', 'Valid_Word'), (',', 'Punc'), ('bÃ¼tÃ¼n', 'Valid_Word'), ('kaleleri', 'Valid_Word'), ('zaptedilmiÅŸ', 'OOV'), (',', 'Punc'), ('bÃ¼tÃ¼n', 'Valid_Word'), ('tersanelerine', 'Valid_Word'), ('girilmiÅŸ', 'Valid_Word'), (',', 'Punc'), ('bÃ¼tÃ¼n', 'Valid_Word'), ('ordularÄ±', 'Valid_Word'), ('daÄŸÄ±tÄ±lmÄ±ÅŸ', 'Valid_Word'), ('ve', 'Valid_Word'), ('memleketin', 'Valid_Word'), ('her', 'Valid_Word'), ('kÃ¶ÅŸesi', 'Valid_Word'), ('bilfiil', 'Valid_Word'), ('iÅŸgal', 'Valid_Word'), ('edilmiÅŸ', 'Valid_Word'), ('olabilir', 'Valid_Word'), ('.', 'Punc')]
[('BÃ¼tÃ¼n', 'Valid_Word'), ('bu', 'Valid_Word'), ('ÅŸeraitten', 'Valid_Word'), ('daha', 'Valid_Word'), ('elÃ®m', 'Valid_Word'), ('ve', 'Valid_Word'), ('daha', 'Valid_Word'), ('vahim', 'Valid_Word'), ('olmak', 'Valid_Word'), ('Ã¼zere', 'Valid_Word'), (',', 'Punc'), ('memleketin', 'Valid_Word'), ('dahilinde', 'Valid_Word'), (',', 'Punc'), ('iktidara', 'Valid_Word'), ('sahip', 'Valid_Word'), ('olanlar', 'Valid_Word'), ('gaflet', 'Valid_Word'), ('ve', 'Valid_Word'), ('dalÃ¢let', 'Valid_Word'), ('ve', 'Valid_Word'), ('hattÃ¢', 'Valid_Word'), ('hÄ±yanet', 'Valid_Word'), ('iÃ§inde', 'Valid_Word'), ('bulunabilirler', 'Valid_Word'), ('.', 'Punc')]
[('Hatta', 'Valid_Word'), ('bu', 'Valid_Word'), ('iktidar', 'Valid_Word'), ('sahipleri', 'Valid_Word'), ('ÅŸahsÃ®', 'Valid_Word'), ('menfaatlerini', 'Valid_Word'), (',', 'Punc'), ('mÃ¼stevlilerin', 'Valid_Word'), ('siyasÃ®', 'Valid_Word'), ('emelleriyle', 'Valid_Word'), ('tevhit', 'Valid_Word'), ('edebilirler', 'Valid_Word'), ('.', 'Punc')]
[('Millet', 'Valid_Word'), (',', 'Punc'), ('fakruzaruret', 'Valid_Word'), ('iÃ§inde', 'Valid_Word'), ('harap', 'Valid_Word'), ('ve', 'Valid_Word'), ('bÃ®tap', 'Valid_Word'), ('dÃ¼ÅŸmÃ¼ÅŸ', 'Valid_Word'), ('olabilir', 'Valid_Word'), ('.', 'Punc'), ('Ey', 'Valid_Word'), ('TÃ¼rk', 'Valid_Word'), ('istikbalinin', 'Valid_Word'), ('evladÄ±', 'Valid_Word'), ('!', 'Punc'), ('Ä°ÅŸte', 'Valid_Word'), (',', 'Punc'), ('bu', 'Valid_Word'), ('ahval', 'Valid_Word'), ('ve', 'Valid_Word'), ('ÅŸerait', 'Valid_Word'), ('iÃ§inde', 'Valid_Word'), ('dahi', 'Valid_Word'), (',', 'Punc'), ('vazifen', 'Valid_Word'), (';', 'Punc'), ('TÃ¼rk', 'Valid_Word'), ('istiklÃ¢l', 'Valid_Word'), ('ve', 'Valid_Word'), ('cumhuriyetini', 'Valid_Word'), ('kurtarmaktÄ±r', 'Valid_Word'), ('!', 'Punc')]
[('MuhtaÃ§', 'Valid_Word'), ('olduÄŸun', 'Valid_Word'), ('kudret', 'Valid_Word'), (',', 'Punc'), ('damarlarÄ±ndaki', 'Valid_Word'), ('asil', 'Valid_Word'), ('kanda', 'Valid_Word'), (',', 'Punc'), ('mevcuttur', 'Valid_Word'), ('!', 'Punc')]
```

## CharFix

CharFix offers methods to correct corrupted Turkish text:

### Fix Characters

```python
from ts_tokenizer.char_fix import CharFix

line = "ParÃƒÂ§a ve bÃƒÂ¼tÃƒÂ¼n iliÃ¥Ã¿kisi her zaman iÃ¥Ã¿levsel deÃ°ildir."
print(CharFix.fix(line))  # Fixes corrupted characters
```
```bash
$ ParÃ§a ve bÃ¼tÃ¼n iliÅŸkisi her zaman iÅŸlevsel deÄŸildir.
```
### Lowercase

```python
from ts_tokenizer.char_fix import CharFix

line = "Ä°stanbul ve IÄŸdÄ±r ''arasÄ±'' 1528 km'dir."
print(CharFix.tr_lowercase(line))
```
```bash
$ istanbul ve Ä±ÄŸdÄ±r ''arasÄ±'' 1528 km'dir.
```
### Fix Quotes

```python
from ts_tokenizer.char_fix import CharFix

line = "Ä°stanbul ve IÄŸdÄ±r ''arasÄ±'' 1528 km'dir."
print(CharFix.fix_quote(line))
```
    $ Ä°stanbul ve IÄŸdÄ±r "arasÄ±" 1528 km'dir.

---
## TokenHandler

TokenHandler gets each given string and process it using methods defined under TokenPreProcess class.
This process follows a strictly defined order and it is recursive.
Each method could be called 
```python
from ts_tokenizer.token_handler import TokenPreProcess

```


### Below are the list of tags generated by tokenizer to process tokens:


| #  | Function                   | Sample                       | Used As Output | Tag                |
|----|----------------------------|------------------------------|----------------|--------------------|
| 01 | is_mention                 | @ts-tokenizer                | Yes            | Mention            |
| 02 | is_hashtag                 | #ts-tokenizer                | Yes            | Hashtag            |
| 03 | is_in_quotes               | "ts-tokenizer"               | No             | -----              |
| 04 | is_numbered_title          | (1)                          | Yes            | Numbered_Title     |
| 05 | is_in_paranthesis          | (bilgisayar)                 | No             | -----              |
| 06 | is_date_range              | 01.01.2024-01.01.2025        | Yes            | Date_Range         |
| 07 | is_complex_punc            | -yeniden,sonradan..          | No             | -----              |
| 08 | is_date                    | 22.02.2016                   | Yes            | Date               |
| 09 | is_hour                    | 14.05                        | Yes            | Hour               |
| 10 | is_percentage_numbers      | %75                          | Yes            | Percentage_Numbers |
| 11 | is_percentage_numbers_chars | %75'lik                      | Yes            | Percentage_Numbers |
| 12 | is_roman_number            | XI                           | Yes            | Roman_Number       |
| 13 | is_bullet_list             | â€¢GiriÅŸ                       | Yes            | Bullet_List        |
| 14 | is_email                   | tanersezerr@gmail.com        | Yes            | Email              |
| 15 | is_email_punc              | tanersezerr@gmail.com.       | No             | -----              |
| 16 | is_full_url                | https://tscorpus.com         | Yes            | Full_URL           |
| 17 | is_web_url                 | www.tscorpus.com             | Yes            | Web_URL            |
| -- | is_full_url                | www.example.com'un           | Yes            | URL_Suffix         |
| 18 | is_copyright               | Â©tscorpus                    | Yes            | Copyright          |
| 19 | is_registered              | tscorpusÂ®                    | Yes            | Registered         |
| 20 | is_trademark               | tscorpusâ„¢                    | Yes            | Trademark          |
| 21 | is_currency                | 100$                         | Yes            | Currency           |
| 22 | is_num_char_sequence       | 380A                         | No             | -----              |
| 23 | is_abbr                    | TBMM                         | Yes            | Abbr               |
| 24 | is_in_lexicon              | bilgisayar                   | Yes            | Valid_Word         |
| 25 | is_in_exceptions           | e-mail                       | Yes            | Exception          |
| 26 | is_in_eng_words            | computer                     | Yes            | English_Word       |
| 27 | is_smiley                  | :)                           | Yes            | Smiley             |
| 28 | is_multiple_smiley         | :):)                         | No             | -----              |
| 29 | is_emoticon                | ğŸ»                           | Yes            | Emoticon           |
| 30 | is_multiple_emoticon       | ğŸ»ğŸ»                         | No             | -----              |
| 31 | is_multiple_smiley_in      | hey:):)                      | No             | -----              |
| 32 | is_number                  | 175.01                       | Yes            | Number             |
| 33 | is_apostrophed             | TÃ¼rkiye'nin                  | Yes            | Apostrophed        |
| 34 | is_single_punc             | !                            | Yes            | Punc               |
| 35 | is_multi_punc              | !!                           | No             | -----              |
| 36 | is_single_hyphenated       | sabah-akÅŸam                  | Yes            | Single_Hyphenated  |
| 37 | is_multi_hyphenated        | Ã§ay-su-kahve                 | Yes            | Multi-Hyphenated   |
| 38 | is_single_underscored      | Gel_Git                      | Yes            | Single_Underscored |
| 39 | is_multi_underscored       | YarÄ±_YapÄ±landÄ±rÄ±lmÄ±ÅŸ_MÃ¼lakat | Yes            | Multi_Underscored  |
| 40 | is_one_char_fixable        | bilgisaÂ¬yar                  | Yes            | One_Char_Fixed     |
| 42 | is_three_or_more           | heyyyyy                      | No             | -----              |
| 43 | is_fsp                     | bilgisayar.                  | No             | -----              |
| 44 | is_isp                     | .bilgisayar                  | No             | -----              |
| 45 | is_fmp                     | bilgisayar..                 | No             | -----              |
| 46 | is_imp                     | ..bilgisayar                 | No             | -----              |
| 47 | is_msp                     | --bilgisayar--               | No             | -----              |
| 48 | is_mssp                    | -bilgisayar-                 | No             | -----              |
| 49 | is_midsp                   | okul,Ã¶ÄŸrenci                 | No             | -----              |
| 50 | is_midmp                   | okul,Ã¶ÄŸrenci, Ã¶ÄŸretmen       | No             | -----              |
| 51 | is_non_latin               | í•œêµ­ë“œ                          | No             | Non_Latin          |

----------------------

## Performance

ts-tokenizer is optimized for efficient tokenization and takes advantage of multi-core processing for large-scale text. By default, the script utilizes all available CPU cores minus one, ensuring your system remains responsive while processing large datasets.

### Performance Benchmarks:

The following benchmarks were conducted on different machines with the following specifications:

| **Processor**                                                            | **Cores**                 | **RAM**      | **1 Million Tokens (Multi-Core)** | **Throughput (Multi-Core)** | **1 Million Tokens (Single-Core)** | **Throughput (Single-Core)** |
|--------------------------------------------------------------------------|---------------------------|--------------|-----------------------------------|-----------------------------|------------------------------------|------------------------------|
| AMD Ryzen 7 5800H with Radeon Graphics (Laptop) <br/>3.2 GHz / 4.4 Ghz   | 8 physical cores (16 threads) | 16GB DDR4   | ~170 seconds                      | ~5,800 tokens/second        | ~715 seconds                     | ~1,400 tokens/second         |
| AMD Ryzen 9 7950X3D with Radeon Graphics (Desktop)<br/>4.2 Ghz / 5.7 Ghz | 16 physical cores (32 threads)| 96GB DDR5   | ~14 seconds                       | ~71,500 tokens/second       | ~110 seconds                    | ~9,090 tokens/second         |


