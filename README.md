# TS Tokenizer

**TS Tokenizer** is a hybrid tokenizer designed specifically for tokenizing Turkish texts.
It uses a hybrid (lexicon-based and rule-based) approach to split text into tokens.


### Key Features:
- **Hybrid Approach**: Uses a hybrid (lexicon-based and rule-based approach) for tokenization.
- **Handling of Special Tokens**: Recognizes special tokens like mentions, hashtags, emails, URLs, numbers, smiley, emoticons, etc.
- **Highly Configurable**: Provides multiple output formats to suit different NLP processing needs, including plain tokens, tagged tokens, and token-tag pairs in list or line formats.

On natural language processing (NLP), information retrieval, or text mining for Turkish, **TS Tokenizer** offers
a reliable solution for tokenization.

---

## Installation

You can install the ts-tokenizer package using pip.

    pip install ts-tokenizer

Ensure you have Python 3.9 or higher installed on your system.

You can update current version using pip

    pip install --upgrade ts-tokenizer
---

## Command line tool

You can use TS Tokenizer directly from the command line for both file inputs and pipeline processing:
## Tokenize from a File:

    $ ts-tokenizer input.txt

or
    
    $ cat input.txt | ts-tokenizer

## Tokenizing with Piped Input:

    $ zcat input.txt.gz | ts-tokenizer

---

## Help

Get detailed help for available options using:
    
    $ ts-tokenizer --help
    
    usage: main.py [-h] [-o {tokenized,lines,tagged,tagged_lines}] [-v] [-n NUM_WORKERS]
                   [filename]
    
    Tokenizer Script
    
    positional arguments:
      filename              Name of the file to process (optional if input is piped)
    
    options:
      -h, --help            show this help message and exit
      -o {tokenized,lines,tagged,tagged_lines}, --output {tokenized,lines,tagged,tagged_lines}
                            Specify the output format
      -v, --verbose         Enable verbose mode
      -n NUM_WORKERS, --num-workers NUM_WORKERS
                            Number of parallel workers

---

## CLI Arguments

You can specify the output format using the -o option:

- **tokenized (default):** Returns plain tokens, one per line.
- **tagged:** Returns tokens with their tags.
- **lines:** Returns tokenized lines as lists.
- **tagged_lines:** Returns tokenized lines as a list of tuples (token, tag).

input_text = "Queen , 31.10.1975 tarihinde Ã§Ä±kardÄ±Ã°Ä± A Night at the Opera albÃƒÂ¼mÃƒÂ¼yle dÃƒÂ¼nya mÃƒÂ¼ziÃ°ini deÃ°iÃ¥Ã¿tirdi ."

    $ ts-tokenizer input text

    Queen
    ,
    31.10.1975
    tarihinde
    Ã§Ä±kardÄ±ÄŸÄ±
    A
    Night
    at
    the
    Opera
    albÃ¼mÃ¼yle
    dÃ¼nya
    mÃ¼ziÄŸini
    deÄŸiÅŸtirdi
    .

Note that **tags are not part-of-speech tags** but they define the given string.
    
    $ ts-tokenizer -o tagged input.txt

    Queen	English_Word
    ,	Punc
    31.10.1975	Date
    tarihinde	Valid_Word
    Ã§Ä±kardÄ±ÄŸÄ±	Valid_Word
    A	OOV
    Night	English_Word
    at	Valid_Word
    the	English_Word
    Opera	Valid_Word
    albÃ¼mÃ¼yle	Valid_Word
    dÃ¼nya	Valid_Word
    mÃ¼ziÄŸini	Valid_Word
    deÄŸiÅŸtirdi	Valid_Word
    .	Punc


The other two arguments are "lines" and "tagged_lines".
The "lines" parameter reads input file line-by-line and returns a list for each line.

    $ ts-tokenizer -o lines input.txt

    ['Queen', ',', '31.10.1975', 'tarihinde', 'Ã§Ä±kardÄ±ÄŸÄ±', 'A', 'Night', 'at', 'the', 'Opera', 'albÃ¼mÃ¼yle', 'dÃ¼nya', 'mÃ¼ziÄŸini', 'deÄŸiÅŸtirdi', '.']

The "tagged_lines" parameter reads input file line-by-line and returns a list of tuples for each line.


    $ ts-tokenizer -o tagged_lines input.txt

    [('Queen', 'English_Word'), (',', 'Punc'), ('31.10.1975', 'Date'), ('tarihinde', 'Valid_Word'), ('Ã§Ä±kardÄ±ÄŸÄ±', 'Valid_Word'), ('A', 'OOV'), ('Night', 'English_Word'), ('at', 'Valid_Word'), ('the', 'English_Word'), ('Opera', 'Valid_Word'), ('albÃ¼mÃ¼yle', 'Valid_Word'), ('dÃ¼nya', 'Valid_Word'), ('mÃ¼ziÄŸini', 'Valid_Word'), ('deÄŸiÅŸtirdi', 'Valid_Word'), ('.', 'Punc')]

---

## Parallel Processing
Use the -n option to set the number of parallel workers:

    $ ts-tokenizer -n 2 -o tagged input_file

By default, TS Tokenizer uses [number of CPU cores - 1].

---

## Using CLI Arguments with pipelines

You can use TS Tokenizer in bash pipelines, such as counting word frequencies:

Following sample returns calculated  frequencies for the given file:

    $ ts-tokenizer input.txt | sort | uniq -c | sort -n

---

To count tags:

    $ts-tokenizer -o tagged input.txt | cut -f2 | sort | uniq -c
        1 Date
        3 English_Word
        2 Hashtag
        2 Mention
        1 Multi_Hyphenated
        3 Numbered_Title
        1 OOV
        9 Punc
        1 Single_Hyphenated
        17 Valid_Word


To find a specific tag following command could be used.

    $ ts-tokenizer -o tagged input.txt | cut -f1,2 | grep "Web_URL"
    www.wikipedia.org	Web_URL
    www.wim-wenders.com	Web_URL
    www.winterwar.com.	Web_URL
    www.wissenschaft.de:	Web_URL
    www.wittingen.de	Web_URL
    www.wlmqradio.com	Web_URL
    www.worldstadiums.com	Web_URL
    www.worldstatesmen.org	Web_URL


---

# Classes

## CharFix

CharFix offers methods to correct corrupted Turkish text:

### Fix Characters

```python
from ts_tokenizer.char_fix import CharFix

line = "ParÃƒÂ§a ve bÃƒÂ¼tÃƒÂ¼n iliÃ¥Ã¿kisi her zaman iÃ¥Ã¿levsel deÃ°ildir."
print(CharFix.fix(line))  # Fixes corrupted characters

```
### Lowercase

```python
from ts_tokenizer.char_fix import CharFix

line = "Ä°stanbul ve IÄŸdÄ±r ''arasÄ±'' 1528 km'dir."
print(CharFix.tr_lowercase(line))
```
    $istanbul ve Ä±ÄŸdÄ±r ''arasÄ±'' 1528 km'dir.

### Fix Quotes

```python
from ts_tokenizer.char_fix import CharFix

line = "Ä°stanbul ve IÄŸdÄ±r ''arasÄ±'' 1528 km'dir."
print(CharFix.fix_quote(line))
```
    $ Ä°stanbul ve IÄŸdÄ±r "arasÄ±" 1528 km'dir.


## Punctuation Check
This method returns information about the punctuations in given word.

## PuncMatcher Class

This class has 6 methods. They return information about punctuations in given string.

### punc_count
This method returns number of punctuations in given string as list

```python
from ts_tokenizer.punctuation_process import PuncMatcher

sample_one = "ornek,"
sample_two = "kalem,kitap,defter"

print(PuncMatcher.punc_count(sample_one))
$1
print(PuncMatcher.punc_count(sample_two))
$3
```
---

### punc_pos
This method returns indexes of punctuations in given string as list

```python
from ts_tokenizer.punctuation_process import PuncMatcher

sample_one = "ornek,"
sample_two = "kalem,kitap,defter"

print(PuncMatcher.punc_pos(sample_one))
$[5]
print(PuncMatcher.punc_pos(sample_two))
$[5, 11]
```

---

## Below are the list of tags generated by tokenizer in their process order:

    is_mention ==> tanersezerr@gmail.com
    is_hashtag ==> #ts-tokenizer
    is_in_quotes ==> "ts-tokenizer"
    is_numbered_title ==> (1)
    is_in_parenthesis ==> (bilgisayar)
    is_date_range ==> 01.01.2024-01.01.2025
    is_complex_punc ==> 
    is_date ==> 22.02.2016
    is_hour ==> 16:35
    is_percentage_numbers ==> %75
    is_percentage_numbers_chars ==> %75'lik
    is_roman_number ==> XI
    is_bullet_list ==> â€¢GiriÅŸ
    is_email_punc ==> tnrszr@gmail.com'dan
    is_email ==> tnrszr@gmail.com
    is_full_url ==> https://tscorpus.com
    is_web_url ==> www.tscorpus.com
    is_copyright ==> Â©tscorpus
    is_registered ==> Â®tscorpus
    is_trademark ==> tscorpusâ„¢
    is_currency ==> 100$
    is_num_char_sequence
    is_abbr ==> TBMM
    is_in_lexicon ==> bilgisayar
    is_in_exceptions ==> e-mail
    is_in_eng_words ==> computer
    is_smiley ==> :)
    is_emoticon ==> ðŸ»
    is_multiple_smiley ==> :):)
    is_multiple_smiley_in ==> hey:):)
    is_multiple_emoticon ==> ðŸ»ðŸ»
    is_number ==> 175.01
    is_fsp ==> bilgisayar.
    is_isp ==> .bilgisayar
    is_mssp ==> -bilgisayar-
    is_msp ==> --bilgisayar-
    is_imp ==> --bilgisayar
    is_fmp ==> bilgisayar??
    is_apostrophed ==> TÃ¼rkiye'nin
    is_single_punc ==> !
    is_multi_punc ==> !!
    is_single_hyphenated ==> gel-git
    is_multi_hyphenated ==> Ã§ay-su-kahve
    is_single_underscored ==> bir_iki
    is_multi_underscored ==> Ã§ay_su_kahve
    is_three_or_more ==> heyyyy
    is_non_latin ==> í•œêµ­ë“œë¼ë§ˆ
    is_one_char_fixable ==> bilgisaÂ¬yar
    is_midsp ==> okul,Ã¶ÄŸrenci
    is_midmp ==> okul,Ã¶ÄŸrenci,Ã¶ÄŸretmen